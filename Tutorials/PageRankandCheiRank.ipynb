{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank and CheiRank Calculated from ScreamingFrog Crawl Data and Moz API Data\n",
    "\n",
    "The following workflow requires two files.\n",
    "\n",
    "* **internal_html.csv**: Is an export of Internal HTML from Screaming Frog. The Moz API must be enabled (requires API key). Select `URL` > `MozRank External Equity` in `API Access` > `Moz` > `Metrics` of Screaming Frog.\n",
    "\n",
    "* **all_inlinks.csv**: Is an bulk export of `Bulk Export` >  `All Inlinks` from Screaming Frog.\n",
    "\n",
    "Both files are raw exports so Column names are the defaults and the read_csv function expects to skip the first row for Screaming Frog 12.4 and earlier (eg. skiprows=1) BUT does not expect to skip the first row for ScreamingFrog 12.5 or later (eg. skiprows=0).\n",
    "\n",
    "A follow up to this [tweet](https://twitter.com/willem_nout/status/1101417508685467648).\n",
    "\n",
    "Follow me, [JR Oakes](https://twitter.com/jroakes), on Twitter for more Technical SEO goodness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install These Libraries\n",
    "\n",
    "If you don't have them.  Otherwise, skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "!pip insall pandas\n",
    "!pip insall tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Some Variables\n",
    "\n",
    "We need to specify some variables we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base domain for your property\n",
    "domain = \"domain.com\"\n",
    "\n",
    "# Specify the output filename base.  Auto-generated from domain if `None`.\n",
    "filename = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Which Consolidate URLs\n",
    "1. Consolidates URLs to canonical versions based on canonical link element.\n",
    "1. Provides mapping dictionary for 30X URls from intial 301 to final 200 (canonical) URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_mapping(row, mapping):\n",
    "    if row['Destination'] in mapping:\n",
    "        row['Destination'] = mapping[row['Destination']]\n",
    "    if row['Source'] in mapping:\n",
    "        row['Source'] = mapping[row['Source']]\n",
    "    return row\n",
    "\n",
    "\n",
    "def consolidate_urls(df_html):\n",
    "    \n",
    "    mappings = {}\n",
    "    \n",
    "    # Consolidate canonicals\n",
    "    good_statuses = [200]\n",
    "    df_html_200 = pd.DataFrame()\n",
    "    \n",
    "    df_html_good = df_html[df_html['Status Code'].isin(good_statuses)]\n",
    "    \n",
    "    for i, row in tqdm(df_html_good.iterrows(), total=df_html_good.shape[0]):\n",
    "        \n",
    "        canonical = str(row['Canonical Link Element 1'])\n",
    "        \n",
    "        if \"/\" in canonical and canonical != row['Address']:\n",
    "            mappings[row['Address']] = canonical\n",
    "            row['Address'] = canonical\n",
    "        else:\n",
    "            mappings[row['Address']] = row['Address']\n",
    "            \n",
    "        df_html_200 = df_html_200.append(row, ignore_index=True)\n",
    "    \n",
    "    df_html_200 = df_html_200.groupby(['Address'], as_index=False).agg({'Moz External Equity Links - Exact': 'sum', 'Outlinks':'max'})\n",
    "    \n",
    "    # Create mapping for redirects\n",
    "    redirect_statuses = [301,302]\n",
    "    df_html_redir = df_html[df_html['Status Code'].isin(redirect_statuses)]\n",
    "    \n",
    "    addresslist = df_html_redir['Address'].tolist()\n",
    "    redirlist =  df_html_redir['Redirect URL'].tolist()\n",
    "    \n",
    "    for i, address in tqdm(enumerate(addresslist)):\n",
    " \n",
    "        redir = redirlist[i]\n",
    "        \n",
    "        if redir in mappings:\n",
    "            mappings[address] = mappings[redir]\n",
    "        else:\n",
    "            for _ in range(5):\n",
    "                if redir in addresslist:\n",
    "                    redir = redirlist[addresslist.index(redir)]\n",
    "                    if redir in mappings:\n",
    "                        mappings[address] = mappings[redir]\n",
    "                        break\n",
    "                        \n",
    "                        \n",
    "    return df_html_200, mappings\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Crawl HTML Data\n",
    "\n",
    "Crawl with Screaming Frog and export HTML.  Ensure that you respect robots.txt, noindex, canonical, etc to try to get as close a representation to what Google gets as possible.\n",
    "\n",
    "**Warning**: Make sure that `URL` > `MozRank External Equity` is selected in `API Access` > `Moz` > `Metrics` of Screaming Frog (requires API key) \n",
    "\n",
    "**Expects**: `internal_html.csv` file from Screaming Frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html = pd.read_csv('internal_html.csv', skiprows=0)\n",
    "\n",
    "# Grab 200 urls and canonicalize\n",
    "df_html, mappings = consolidate_urls(df_html)\n",
    "\n",
    "df_html = df_html[['Address','Moz External Equity Links - Exact', 'Outlinks']]\n",
    "df_html.columns = ['Address', 'Equity', 'Outlinks']\n",
    "df_html.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Internal Link Data\n",
    "\n",
    "After the prior crawl, use the Bulk Export tool to export All Inlinks. We then clean the data a bit to ensure we have only the links that we want.\n",
    "\n",
    "**Expects**: `all_inlinks.csv` file from Screaming Frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv('all_inlinks.csv', skiprows=0, low_memory=False)\n",
    "\n",
    "# keep only Ahref and Follow\n",
    "df_links = df_links[(df_links['Type'] == \"AHREF\") & (df_links['Follow'] == True)]\n",
    "\n",
    "# keep only internal links\n",
    "df_links = df_links[(df_links['Destination'].str.match(r'^(http:|https:)//(www.)?{}.*$'.format(domain), case=False)) & (df_links['Source'].str.match(r'^(http:|https:)//(www.)?{}.*$'.format(domain), case=False))]\n",
    "\n",
    "# Map links to their final destination\n",
    "df_links = df_links.apply(apply_mapping, axis=1, args=(mappings,))\n",
    "\n",
    "# Keep only the columns we need\n",
    "df_links = df_links[['Source','Destination']]\n",
    "\n",
    "df_links.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Links in Both Datasets\n",
    "Converts the urls to paths and removes trailing slashes.  \n",
    "\n",
    "**Warning**: This is not really needed as the consolidation done earlier is more reflective of how Google handles URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def remove_trail_slash(s):\n",
    "    if s.endswith('/'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "# This may or may not be what you want to do depending on the site and can, for sure be extended to keep important querystings or consolidate canoncicals.\n",
    "def apply_clean_links(row):\n",
    "    \n",
    "    cols=['Address', 'Source', 'Destination']\n",
    "    \n",
    "    for c in cols:\n",
    "        if c in row:\n",
    "            row[c] = remove_trail_slash(urlparse(row[c]).path)\n",
    "        \n",
    "    return row\n",
    "\n",
    "\n",
    "df_links = df_links.apply(apply_clean_links, axis=1)\n",
    "df_html = df_html.apply(apply_clean_links, axis=1)\n",
    "\n",
    "#Consolidate External Equity\n",
    "df_html = df_html.groupby([ 'Address'], as_index=False).agg({\"Equity\": \"max\",\"Outlinks\":\"max\"})\n",
    "\n",
    "\n",
    "df_html.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Graphs\n",
    "This sets up the directed graphs used in the PR and CR algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_dataframe(df, addresses, graph, gtype= \"PR\"):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        # Add nodes\n",
    "        if 'Address' in row:\n",
    "            if graph.has_node(row['Address']) == False:\n",
    "                graph.add_node(row['Address'])\n",
    "        # Add edges\n",
    "        elif 'Destination' in row and 'Source' in row:\n",
    "            \n",
    "            #Skip adding edge if source or destination is not in set of pages.\n",
    "            if row['Destination'] in addresses and row['Source'] in addresses:\n",
    "                if gtype == 'PR':\n",
    "                    graph.add_edge(row['Source'], row['Destination'])\n",
    "                else:\n",
    "                    graph.add_edge(row['Destination'], row['Source'])\n",
    "            \n",
    "        else:\n",
    "            raise Exception('The correct dataframes were not supplied.  Expecting either `Address` or `Destination` and `Source` columns.')\n",
    "            \n",
    "            \n",
    "\n",
    "def run_graphs(df_links, df_html):\n",
    "\n",
    "    pr_graph = nx.DiGraph()\n",
    "    cr_graph = nx.DiGraph()\n",
    "    \n",
    "    addresses = df_html['Address'].tolist()\n",
    "    \n",
    "    # Pagerank Graph\n",
    "    traverse_dataframe(df_html, addresses, pr_graph, gtype= \"PR\")\n",
    "    traverse_dataframe(df_links, addresses, pr_graph, gtype= \"PR\")\n",
    "                  \n",
    "    # CheiRank Graph\n",
    "    traverse_dataframe(df_html, addresses, cr_graph, gtype= \"CR\")\n",
    "    traverse_dataframe(df_links, addresses, cr_graph, gtype= \"CR\")\n",
    "    \n",
    "    \n",
    "    return pr_graph, cr_graph\n",
    "    \n",
    "    \n",
    "\n",
    "pr_graph, cr_graph = run_graphs(df_links, df_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get initial weights from Moz External Equity and run PageRank and CheiRank\n",
    "This does all the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr= df_html['Address'].tolist()\n",
    "eqt= df_html['Equity'].tolist()\n",
    "\n",
    "init_nstart = {v:eqt[i] for i,v in enumerate(adr)}\n",
    "\n",
    "scores_pr = nx.pagerank(pr_graph, nstart=init_nstart, max_iter=1000)\n",
    "scores_cr = nx.pagerank(cr_graph, nstart=init_nstart, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PageRank and CheiRank Graph\n",
    "**Warning**: This will more than likely run out of memory or be hard to read for large sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "topn = 10\n",
    "\n",
    "if scores_pr:\n",
    "    # Sort nodes by best\n",
    "    ranked_nodes_pr = sorted(((scores_pr[s],s) for i,s in enumerate(list(scores_pr.keys()))), reverse=True)\n",
    "    # Get the topn nodes\n",
    "    nodelist = [n[1] for n in ranked_nodes_pr][:topn]\n",
    "    edgelist = [(a[0],a[1]) for a in pr_graph.edges() if a[0] in nodelist and a[1] in nodelist]\n",
    "    labels = {n:n for n in nodelist}\n",
    "    sizes_pr = [(scores_pr[x])*10000 for x in list(scores_pr) if x in nodelist]   \n",
    "    sm = nx.draw(pr_graph, with_labels = True, node_size=sizes_pr, nodelist=nodelist, edgelist=edgelist, labels=labels)\n",
    "    plt.show()\n",
    "    \n",
    "if scores_cr:\n",
    "    # Sort nodes by best\n",
    "    ranked_nodes_cr = sorted(((scores_cr[s],s) for i,s in enumerate(list(scores_cr.keys()))), reverse=True)\n",
    "    # Get the topn nodes\n",
    "    nodelist = [n[1] for n in ranked_nodes_cr][:topn]\n",
    "    edgelist = [(a[0],a[1]) for a in cr_graph.edges() if a[0] in nodelist and a[1] in nodelist]\n",
    "    labels = {n:n for n in nodelist}\n",
    "    sizes_cr = [(scores_cr[x])*10000 for x in list(scores_cr) if x in nodelist]   \n",
    "    sm = nx.draw(cr_graph, with_labels = True, node_size=sizes_cr, nodelist=nodelist, edgelist=edgelist, labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a CSV\n",
    "Saves the initial normalized data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scores(row,scores_pr,scores_cr):\n",
    "    adr = row['Address']\n",
    "    otl = int(row['Outlinks'] or 1)\n",
    "    row['PageRank'] = scores_pr.get(adr,0)\n",
    "    row['CheiRank'] = scores_cr.get(adr,0)\n",
    "    row['Link Equity'] = float(scores_cr.get(adr,0)/otl)\n",
    "   \n",
    "    return row\n",
    "\n",
    "def normalize_colums(df):\n",
    "    cols = ['PageRank','Equity','CheiRank','Link Equity']\n",
    "    \n",
    "    for c in cols:\n",
    "        df[c] = (df[c]-df[c].min())/(df[c].max()-df[c].min())\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_html = df_html.apply(apply_scores, args=(scores_pr,scores_cr), axis=1)\n",
    "df_html_norm = normalize_colums(df_html)\n",
    "\n",
    "fname = 'norm_' + (filename or domain.replace('.','_') + \".csv\")\n",
    "\n",
    "df_html_norm.to_csv(fname)\n",
    "df_html_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Categories\n",
    "This apply function can be adjusted to apply category groupings to your data however you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cat(row):\n",
    "    address = row[\"Address\"]\n",
    "    row['Category'] = \"None\"\n",
    "    \n",
    "    address_parts = re.sub(r'^https?:\\/\\/(www\\.)?{}/'.format(domain), \"\", address).split('/')\n",
    "    \n",
    "    # Adjust below to categorize how you like. \n",
    "    \n",
    "    if len(address_parts) > 2:\n",
    "        row['Category'] = address_parts[0]+\"-\"+address_parts[1]\n",
    "    elif len(address_parts) > 0:\n",
    "        row['Category'] = address_parts[0]\n",
    "            \n",
    "    return row\n",
    "\n",
    "df_html_norm = df_html_norm.apply(apply_cat, axis=1)\n",
    "\n",
    "fname = 'cat_norm_' + (filename or domain.replace('.','_') + \".csv\")\n",
    "\n",
    "df_html_norm.to_csv(fname)\n",
    "df_html_norm.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PythonforSEOTechSEOBoost2018_Hamlet_Batista.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Ldi4r5rieHr7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "\n",
        "First we import some of the libraries that we're going to use throughout this exercise:\n",
        "\n",
        "\n",
        "### Pandas\n",
        "[Pandas](https://pandas.pydata.org/) is a library designed to help with data analysis. It includes tools for organizing data, doing common analysis tasks like groups, joins, sums, and averages. It's built on top of another common (and more advanced) library called [numpy](http://www.numpy.org/) which enables extremely fast numerical computations. In general, if you can find a way to do what you want to do in pandas/numpy, it's going to be much faster than anything you're likely to code up.\n",
        "\n",
        "### Plotly\n",
        "[Plotly](https://plot.ly/python/) is a library for making pretty interactive graphs.\n",
        "\n",
        "### Requests\n",
        "[Requests](http://docs.python-requests.org/en/master/), as the name might suggest, makes HTTP web requests easy, earning it the tongue-in-cheek descriptor: \"HTTP for humans\".\n",
        "\n",
        "### google.colab\n",
        "[A library designed specifically for the google.colab coding environment](https://colab.research.google.com/notebooks/io.ipynb) in which this notebook runs. We'll use it for accessing files within our own google drive folders, but it can do a bunch of other stuff too.\n",
        "\n",
        "\n",
        "### urllib.parse\n",
        "\n",
        "A python [standard library module](https://docs.python.org/3/library/urllib.parse.html), from which we'll use the \"urlparse\" function only. As the name implies it allows us to easily parse the different parts of a URL (path, parameters, protocol, etc).\n",
        "\n",
        "### re\n",
        "\n",
        "Another member of the [standard library](https://docs.python.org/3/library/re.html) that provides a number of functions that help with using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).\n",
        "\n",
        "### time\n",
        "Standard library module that handles time, and for our purposes provides us with a sleep function.\n",
        "\n",
        "### Ipython.display\n",
        "We'll import the HTML function, which allows us to display HTML files in the ipython notebook (or in this case, the google colab notebook) from the filesystem.\n",
        "\n",
        "\n",
        "### Sklearn\n",
        "\n",
        "[Sklearn](https://scikit-learn.org/stable/) is one of the most popular libraries for [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) in python (and in general). It implements everything from simple linear regression, to Support Vector Machines, to even small neural nets. \n",
        "\n",
        "Sklearn makes complicated machine learning algorithms accessible to everyone, and also has a number of functions for making sure you're practicing [good experiemental design](https://en.wikipedia.org/wiki/Cross-validation_(statistics%29), and [evaluating how good](https://en.wikipedia.org/wiki/Confusion_matrix) the models you've trained are."
      ]
    },
    {
      "metadata": {
        "id": "d_KVjtywIPG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import files, drive\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rg-zZPRkqzB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Files inside Google Colab\n",
        "\n",
        "## Local Filesystem\n",
        "\n",
        "If you click on the right-facing arrow at the top right of the google colab screen, a window pane will open up that has 3 tabs in it: \"Table of Contents\", \"Code snippets\", and \"Files\". \n",
        "\n",
        "Clicking on the \"Files\" tab you'll be able to see what files we can access with our code. Folders and Files listed here act just like they do on your computer, you can even right click on any of the files you see and select \"download\" to download them permanently to your computer. \n",
        "\n",
        "With the exception of the \"sample_data\" folder that appears in every google colab notebook, files uploaded here are ephemeral: they will disappear when you close the colab notebook, and you'll have to upload them again next time you want to use them in your code.\n",
        "\n",
        "\n",
        "## Google Drive\n",
        "\n",
        "Some of the files we'll generate from the code in this notebook are okay to throw away, like the HTML files that hold our plotly graphs. Some of them, however, we would like to save permanently because they take a lot of time to create, and we don't want to have to wait for them to get created again every time we want to play around with the code.\n",
        "\n",
        "To solve this problem, google provides a way for us to access all the files we have in our Google Drive: \n",
        "\n",
        "1. Run the command in the cell below: `drive.mount(\"/content/gdrive\")` you'll be presented with a URL to click, and a text box. \n",
        "2. Clicking the link will open a new tab and a page will appear asking you to give permission to access your google drive. Accept the permission and you'll be shown a string of text (called a \"token\"). \n",
        "3. Copy that token and navigate back here to the notebook, and paste the token in the text box provided earlier. \n",
        "4. Once you've pasted the token hit enter to confirm and wait a few seconds and your Google Drive should be mounted to the google colab notebook.\n",
        "5. Click on the \"refresh\" button in the files tab on the left, you should now see a folder called \"gdrive\" with a sub-folder called \"My Drive\". You'll find all the files you have in your google drive within that folder.\n",
        "\n",
        "You can now treat the gdrive/My Drive/ folder as a permanent storage space for any files you generate in the code below."
      ]
    },
    {
      "metadata": {
        "id": "d99UMR449Dn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbA4osND6DYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution 1 \n",
        "\n",
        "## Part 1: Winners and Losers\n",
        "\n",
        "### Getting The Data\n",
        "We want to analyze some traffic data from google analytics, and we're going to use the [Google Analytics API]() to do it. \n",
        "\n",
        "Google, like most sites out there nowadays, requires us to verify that we are who we say we are before we can access our google analytics data. In short, we need a temporary password that lets google know that our data requests are legit. \n",
        "\n",
        "How do we get this temporary password? Well, we're going to cheat a tiny bit by getting it from the [Google Query Explorer](). The \"correct\" way to do this is beyond the scope of this tutorial. For now: if it works, it works!\n",
        "\n",
        "1. Head on over to the Query Explorer\n",
        "2. Click on the button at the top that says \"Click here to Authorize\" and follow the steps provided.\n",
        "2. Use the dropdown menu to select the website you want to get data from\n",
        "3. Don't worry too much most of the parameters, some will be filled in for you. The only one you need to fill in for now is the \"metrics\" parameter.\n",
        "  * Select any parameter at this point, we only want to be able to run the query explorer once in order to get the token we need. I selected \"**Users**\" since it was at the top of the list.\n",
        "4. Hit \"Run Query\" and let it run\n",
        "5. Scroll down to the bottom of the page and look for the text-box that says \"API Query URI\". \n",
        "  * Check the box underneath it that says \"*Include current access_token in the Query URI (will expire in ~60 minutes).*\"\n",
        "  * At the end of the URL in the text box you should now see access_token=string-of-text-here. Copy that string of text and paste it below in the variable called `token` (make sure to paste it inside the quotes)\n",
        "  \n",
        "6. Now, scroll back up to where we built the query, and look for the parameter that was filled in for you called \"**ids**\". Copy the value in this box and paste it in the variable below called `gaid`. Again, inside the quotes.\n",
        "\n",
        "7. Run the cell once you've filled in the `gaid` and `token` variables to instantiate them, and we're good to go!"
      ]
    },
    {
      "metadata": {
        "id": "b0i7DGHiIbUQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics = \",\".join([\"ga:users\",\"ga:newUsers\"])\n",
        "dimensions = \",\".join([\"ga:landingPagePath\", \"ga:date\"])\n",
        "segment = \"gaid::-5\"\n",
        "\n",
        "\n",
        "# Required, please fill in with your own GA information example: ga:23322342\n",
        "gaid = \"\"\n",
        "\n",
        "# Example: ya29.GltOBqPcbInIS41UouaCztVeYt_-bYyvsb5bs2Du_vz62r21yhEBveNPK-D3k4Du1WVmbAw-zIt8gyjjLAHN4HiWBiEY0IONIwJ_e2swrDK4aZUkBu5ZoVw4nvjS\n",
        "token = \"\"\n",
        "\n",
        "# Example https://www.example.com or http://example.org\n",
        "base_site_url = \"\"\n",
        "\n",
        "\n",
        "# You can change the start and end dates as you like\n",
        "start = \"2017-06-01\"\n",
        "end = \"2018-06-30\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_EvfOnatHm8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following functions use the variables we filled in above to get google analytics data. The specifics of how this works is left as an exercise for the reader, but if you need more information you might try looking [here](https://developers.google.com/analytics/devguides/reporting/core/v3/reference#q_summary)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bFsiyOytNfYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GAData(gaid, start, end, metrics, dimensions, \n",
        "           segment, token, max_results=10000):\n",
        "  \"\"\"Creates a generator that yields GA API data \n",
        "     in chunks of size `max_results`\"\"\"\n",
        "  \n",
        "  #build uri w/ params\n",
        "  api_uri = \"https://www.googleapis.com/analytics/v3/data/ga?ids={gaid}&\"\\\n",
        "             \"start-date={start}&end-date={end}&metrics={metrics}&\"\\\n",
        "             \"dimensions={dimensions}&segment={segment}&access_token={token}&\"\\\n",
        "             \"max-results={max_results}\"\n",
        "  \n",
        "  # insert uri params\n",
        "  api_uri = api_uri.format(\n",
        "      gaid=gaid,\n",
        "      start=start,\n",
        "      end=end,\n",
        "      metrics=metrics,\n",
        "      dimensions=dimensions,\n",
        "      segment=segment,\n",
        "      token=token,\n",
        "      max_results=max_results\n",
        "  )\n",
        "  \n",
        "  # Using yield to make a generator in an\n",
        "  # attempt to be memory efficient, since data is downloaded in chunks\n",
        "  r = requests.get(api_uri)\n",
        "  data = r.json()\n",
        "  yield data\n",
        "  if data.get(\"nextLink\", None):\n",
        "    while data.get(\"nextLink\"):\n",
        "      new_uri = data.get(\"nextLink\")\n",
        "      new_uri += \"&access_token={token}\".format(token=token)\n",
        "      r = requests.get(new_uri)\n",
        "      data = r.json()\n",
        "      yield data\n",
        "\n",
        "      \n",
        "def to_df(gadata):\n",
        "  \"\"\"Takes in a generator from GAData() \n",
        "     creates a dataframe from the rows\"\"\"\n",
        "  \n",
        "  df = None\n",
        "  for data in gadata:\n",
        "    if df is None:\n",
        "      df = pd.DataFrame(\n",
        "          data['rows'], \n",
        "          columns=[x['name'] for x in data['columnHeaders']]\n",
        "      )\n",
        "    else:\n",
        "      newdf = pd.DataFrame(\n",
        "          data['rows'], \n",
        "          columns=[x['name'] for x in data['columnHeaders']]\n",
        "      )\n",
        "      df = df.append(newdf)\n",
        "    print(\"Gathered {} rows\".format(len(df)))\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_GIXyySihe6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we first try to see if the traffic data already exists in our google drive\n",
        "# Otherwise we call the Analytics API to request the data with the above functions\n",
        "# We don't want to have to call the Analytics API if we don't have to.\n",
        "\n",
        "\n",
        "try:\n",
        "  data = pd.read_csv(\"/content/gdrive/My Drive/site_traffic.csv\")\n",
        "except FileNotFoundError:\n",
        "  data = GAData(gaid=gaid, metrics=metrics, start=start, \n",
        "                end=end, dimensions=dimensions, segment=segment, \n",
        "                token=token)\n",
        "  \n",
        "  data = to_df(data)\n",
        "  # If we do end up calling the API for the data: \n",
        "  # we then save it, so we don't have to again\n",
        "  data.to_csv(\"/content/gdrive/My Drive/site_traffic.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hF1dm_0_-Ybr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data['path'] = data['ga:landingPagePath'].apply(lambda x: urlparse(x).path)\n",
        "data['url'] = base_site_url + data['path']\n",
        "data['ga:date'] = pd.to_datetime(data['ga:date'])\n",
        "data['ga:users'] = pd.to_numeric(data['ga:users'])\n",
        "data['ga:newUsers'] = pd.to_numeric(data['ga:newUsers'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u_M3D1r_imjp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "before_shopify = data[data['ga:date'] < pd.to_datetime(\"2017-12-15\")]\n",
        "after_shopify = data[data['ga:date'] >= pd.to_datetime(\"2017-12-15\")]\n",
        "\n",
        "\n",
        "# Traffic totals before shopify switch\n",
        "totals_before = before_shopify[[\"ga:landingPagePath\", \"ga:newUsers\"]]\\\n",
        "                .groupby(\"ga:landingPagePath\").sum()\n",
        "\n",
        "totals_before = totals_before.reset_index()\\\n",
        "                .sort_values(\"ga:newUsers\", ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "# Traffic totals after shopify switch\n",
        "totals_after = after_shopify[[\"ga:landingPagePath\", \"ga:newUsers\"]]\\\n",
        "               .groupby(\"ga:landingPagePath\").sum()\n",
        "\n",
        "totals_after = totals_after.reset_index()\\\n",
        "               .sort_values(\"ga:newUsers\", ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ScCkrXh9nxuC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Comparing pages from before and after the switch\n",
        "change = totals_after.merge(totals_before, \n",
        "                            left_on=\"ga:landingPagePath\", \n",
        "                            right_on=\"ga:landingPagePath\", \n",
        "                            suffixes=[\"_after\", \"_before\"], \n",
        "                            how=\"outer\")\n",
        "\n",
        "change.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "change['difference'] = change['ga:newUsers_after'] - change['ga:newUsers_before']\n",
        "change['percent_change'] = change['difference'] / change['ga:newUsers_before']\n",
        "\n",
        "\n",
        "winners = change[change['percent_change'] > 0]\n",
        "losers = change[change['percent_change'] < 0]\n",
        "no_change = change[change['percent_change'] == 0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "niZ-dLXk_91m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checking that the total traffic adds up\n",
        "data['ga:newUsers'].sum() == change[['ga:newUsers_after', 'ga:newUsers_before']].sum().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGDMWcDbkwF8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "losers.sort_values(\"difference\").head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k96HGY9t6ILS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution 1b"
      ]
    },
    {
      "metadata": {
        "id": "CHMZqY0SrYBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_redirects(url):\n",
        "  try:\n",
        "    r = requests.head(url, stream=True)\n",
        "  except:\n",
        "    return (url, None, \"Error\")\n",
        "  if r.status_code in [301, 302, 307]:\n",
        "    return (url, r.status_code, r.headers['Location'])\n",
        "  elif r.status_code == 404:\n",
        "    return (url, r.status_code, None)\n",
        "  else:\n",
        "    return (url, r.status_code, None)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ds9CXT1c5I8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "results = []\n",
        "def crawl_redirects(urls, sleep_time=.15):\n",
        "  global results\n",
        "  for i, url in enumerate(urls):\n",
        "    result = get_redirects(url)\n",
        "    results.append(result)\n",
        "    if i % 1000 == 0:\n",
        "      print(i,\":\", result)\n",
        "    time.sleep(sleep_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqrnLZQIHZyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dLPjIy5veNtg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  redirects = pd.read_csv(\"/content/gdrive/My Drive/site_redirects.csv\")\n",
        "except FileNotFoundError:\n",
        "  results = crawl_redirects(data['url'].unique().tolist())\n",
        "  redirects = pd.DataFrame(results, columns=[\"url\", \"status_code\", \"redirect_url\"]).drop_duplicates()\n",
        "  redirects.to_csv(\"/content/gdrive/My Drive/site_redirects.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2e6ZwIo7CgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_redirects = data.merge(redirects, left_on=\"url\", right_on=\"url\", how=\"outer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-XoM8oYN554",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_redirects['true_url'] = data_redirects['redirect_url'].combine_first(data_redirects['path'])\n",
        "data_redirects['true_url'] = data_redirects['true_url'].apply(lambda x: urlparse(x).path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96-G910aONXW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "true_before = data_redirects[data_redirects['ga:date'] < pd.to_datetime(\"2017-12-15\")]\n",
        "true_after = data_redirects[data_redirects['ga:date'] >= pd.to_datetime(\"2017-12-15\")]\n",
        "\n",
        "\n",
        "# Traffic totals before shopify switch\n",
        "true_totals_before = true_before[[\"true_url\", \"ga:newUsers\"]]\\\n",
        "                     .groupby(\"true_url\").sum()\n",
        "\n",
        "true_totals_before = true_totals_before.reset_index()\\\n",
        "                     .sort_values(\"ga:newUsers\", ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Traffic totals after shopify switch\n",
        "true_totals_after = true_after[[\"true_url\", \"ga:newUsers\"]]\\\n",
        "                    .groupby(\"true_url\").sum()\n",
        "\n",
        "true_totals_after = true_totals_after.reset_index()\\\n",
        "                    .sort_values(\"ga:newUsers\", ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "My68YstWOOeK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Comparing pages from before and after the switch\n",
        "true_change = true_totals_after.merge(true_totals_before, \n",
        "                            left_on=\"true_url\", \n",
        "                            right_on=\"true_url\", \n",
        "                            suffixes=[\"_after\", \"_before\"], \n",
        "                            how=\"outer\")\n",
        "\n",
        "true_change.loc[:, [\"ga:newUsers_after\", \"ga:newUsers_before\"]].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "true_change['difference'] = true_change['ga:newUsers_after'] - true_change['ga:newUsers_before']\n",
        "true_change['percent_change'] = true_change['difference'] / true_change['ga:newUsers_before']\n",
        "\n",
        "\n",
        "true_winners = true_change[true_change['percent_change'] > 0]\n",
        "true_losers = true_change[true_change['percent_change'] < 0]\n",
        "true_no_change = true_change[true_change['percent_change'] == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jjowjvXW_EB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checking again that the total traffic adds up\n",
        "true_change[[\"ga:newUsers_before\", \"ga:newUsers_after\"]].sum().sum() == data['ga:newUsers'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6AdoF8c5_fkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQTi-qPu6Qgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution 2"
      ]
    },
    {
      "metadata": {
        "id": "HIzmDylIB2PZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ^/collections/.*/products\n",
        "# ^/collections(?!.*/products.*)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHLa0iz2KgmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_redirects['group'] = \"N/A\"\n",
        "data_redirects.loc[data_redirects['true_url'].str.contains(r\"/collections(?!.*/products.*)(?!.*/product.*)\"), \"group\"] = \"Collections\"\n",
        "data_redirects.loc[data_redirects['true_url'].str.contains(r\".*/products/.*|.*/product/.*\"), \"group\"] = \"Products\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LTpJ44Cascj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_data = data_redirects[['group', \"ga:newUsers\", \"ga:date\"]].groupby([\"group\", \"ga:date\"]).sum().reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkgFlSJfatVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_before = grouped_data[grouped_data['ga:date'] < pd.to_datetime(\"2017-12-15\")]\n",
        "grouped_after = grouped_data[grouped_data['ga:date'] >= pd.to_datetime(\"2017-12-15\")]\n",
        "\n",
        "grouped_before_total = grouped_before[[\"group\", \"ga:newUsers\"]].groupby(\"group\").sum().reset_index()\n",
        "grouped_after_total = grouped_after[[\"group\", \"ga:newUsers\"]].groupby(\"group\").sum().reset_index()\n",
        "\n",
        "grouped_change = grouped_before_total.merge(grouped_after_total, left_on=\"group\", right_on=\"group\", suffixes=[\"_before\", \"_after\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PU4RzcIbehWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_change['difference'] = grouped_change['ga:newUsers_after'] - grouped_change['ga:newUsers_before']\n",
        "grouped_change['percent_change'] = grouped_change['difference'] / grouped_change['ga:newUsers_before']\n",
        "grouped_change.sort_values(\"difference\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3EnpMkKPfbs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plot_data = [\n",
        "    go.Bar(\n",
        "        x = grouped_change['group'].tolist(),\n",
        "        y = grouped_change['difference'].tolist(),\n",
        "        marker = dict(\n",
        "          color = 'red'\n",
        "        ),\n",
        "        name = 'Traffic Difference'\n",
        "    ),\n",
        "    go.Bar(\n",
        "      x = grouped_change['group'],\n",
        "      y = grouped_change['ga:newUsers_before'],\n",
        "      marker = dict(\n",
        "        color = 'blue'\n",
        "      ),\n",
        "      name = \"Traffic Before\"\n",
        "    ),\n",
        "    go.Bar(\n",
        "      x = grouped_change['group'],\n",
        "      y = grouped_change['ga:newUsers_after'],\n",
        "      marker = dict(\n",
        "        color = 'orange'\n",
        "      ),\n",
        "      name = \"Traffic After\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "fig = go.Figure(data=plot_data)\n",
        "py.plot(fig, filename=\"base-bar.html\")\n",
        "\n",
        "\n",
        "\n",
        "HTML(filename=\"./base-bar.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNYG5PZaWmNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "line_data = []\n",
        "\n",
        "for group in grouped_data['group'].unique().tolist():\n",
        "  line = go.Scatter(\n",
        "    x = grouped_data.loc[grouped_data['group'] == group, 'ga:date'],\n",
        "    y = grouped_data.loc[grouped_data['group'] == group, 'ga:newUsers'],\n",
        "    name = group,\n",
        "    mode=\"lines\"\n",
        "  )\n",
        "  \n",
        "  line_data.append(line)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "layout = go.Layout(\n",
        "    shapes=\n",
        "     [\n",
        "        {\n",
        "            'type': 'line',\n",
        "            'x0': \"2017-12-15\",\n",
        "            'y0': 0,\n",
        "            'x1': \"2017-12-15\",\n",
        "            'y1': max(grouped_data['ga:newUsers'])+200,\n",
        "            'line': {\n",
        "                'color': 'grey',\n",
        "                'width': .5,\n",
        "             },\n",
        "        },\n",
        "        {\n",
        "            'type': 'line',\n",
        "            'x0': \"2018-05-15\",\n",
        "            'y0': 0,\n",
        "            'x1': \"2018-05-15\",\n",
        "            'y1': max(grouped_data['ga:newUsers'])+200,\n",
        "            'line': {\n",
        "                'color': 'grey',\n",
        "                'width': .5,\n",
        "             },\n",
        "        }\n",
        "     ],\n",
        "                   \n",
        "    annotations = [dict(\n",
        "        showarrow = True,\n",
        "        x = \"2017-12-15\",\n",
        "        y = 1000,\n",
        "        text = \"Shopify Switch\",\n",
        "        xanchor = \"left\",\n",
        "        ax=50,\n",
        "        opacity = 1\n",
        "      ),\n",
        "      dict(\n",
        "        showarrow = True,\n",
        "        x = \"2018-05-15\",\n",
        "        y = 1000,\n",
        "        text = \"???\",\n",
        "        xanchor = \"left\",\n",
        "        ax=-50,\n",
        "        opacity = 1\n",
        "      )]\n",
        ")\n",
        "       \n",
        "  \n",
        "  \n",
        "  \n",
        "fig = go.Figure(data=line_data, layout=layout)\n",
        "py.plot(fig, filename=\"base-line.html\")\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(filename=\"./base-line.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dN-xxa1RmipT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBBC-e4HO9Uq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution 3"
      ]
    },
    {
      "metadata": {
        "id": "LnbuWVXknLNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn as sk\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzFV1djYocZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "urls = data_redirects.loc[~data_redirects['true_url'].str.match(\"Error\"), \"true_url\"]\n",
        "urls = pd.DataFrame(urls.unique(), columns=[\"url\"])\n",
        "\n",
        "\n",
        "# Only taking 50 for development purposes\n",
        "urls.to_csv(\"./crawl_urls.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ESdMSLU3SaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import ImageFile\n",
        "\n",
        "\n",
        "def geturl(url):\n",
        "  \n",
        "  scheme, host, path, params, query, fragment = urlparse(url)\n",
        "  if not path:\n",
        "      path = \"/\"\n",
        "  if params:\n",
        "      path = path + \";\" + params\n",
        "  if query:\n",
        "      path = path + \"?\" + query\n",
        "\n",
        "  url = host + path\n",
        "  \n",
        "  return \"http://\" + url\n",
        " \n",
        "\n",
        "def getsizes(uri):\n",
        "    # get file size *and* image size (None if not known)\n",
        "    with requests.get(uri, stream=True) as file:\n",
        "      size = file.headers.get(\"content-length\")\n",
        "      if size: \n",
        "        size = int(size)\n",
        "      p = ImageFile.Parser()\n",
        "      data = file.iter_content(chunk_size=512)\n",
        "      for datum in data:\n",
        "        p.feed(datum)\n",
        "        if p.image:\n",
        "          return size, p.image.size\n",
        "    return size, None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UuZQLcYo3azd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_counts = pd.read_csv(\"/content/gdrive/My Drive/img_sizes.csv\", index_col=0)\n",
        "form_counts = pd.read_csv(\"/content/gdrive/My Drive/form_counts.csv\", index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2qpIrgZ6H4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 0 - 1000\n",
        "# 1001 - 2000\n",
        "# 2001 - 3000\n",
        "\n",
        "def img_size_group(size):\n",
        "  max_size = 50000\n",
        "  img_size_groups = [i for i in \n",
        "                      zip(\n",
        "                        [i for i in range(0, max_size, 1000)], \n",
        "                        [i for i in range(1000, max_size, 1000)]\n",
        "                      )\n",
        "                    ]\n",
        "  \n",
        "  for lower, upper in img_size_groups:\n",
        "    if size > max_size:\n",
        "      return str(max_size)+\"+\"\n",
        "    elif lower < size < upper:\n",
        "      return \"{}-{}\".format(lower, upper)\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOjRkFS19RS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_counts['filesize_group'] = img_counts['filesize'].apply(img_size_group)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNbeVXgDcryk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "onehot_img = img_counts[['url']].join(pd.get_dummies(img_counts['filesize_group'], drop_first=True))\n",
        "onehot_img = onehot_img.groupby(\"url\").sum().reset_index()\n",
        "onehot_img = img_counts[[\"url\"]].merge(onehot_img, on=\"url\", how=\"left\").drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ioiRzlVTdEWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = form_counts.merge(onehot_img, on=\"url\")\n",
        "data.loc[:, 'group'] = \"N/A\"\n",
        "data.loc[data['url'].str.contains(r\".*/products/.*|.*/product/.*\"), \"group\"] = \"Products\"\n",
        "data.loc[data['url'].str.contains(r\"/collections(?!.*/products.*)(?!.*/product.*)\"), \"group\"] = \"Category\"\n",
        "\n",
        "# data = data.loc[data['group'] == \"Category\", :].append(data.loc[data['group'] == \"Products\", :].sample(500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRwlQ20MeSCR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.axes().grid(b=None)\n",
        "     \n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "#     plt.tight_layout()\n",
        "    \n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(15,12)\n",
        "  \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"group\", \"url\"], axis=1), data['group'], test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gITyRtOqeuzN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "names = [\n",
        "         \"Naive Bayes\",\n",
        "         \"Linear SVM\",\n",
        "         \"Logistic Regression\",\n",
        "         \"Random Forest\",\n",
        "         \"Multilayer Perceptron\"\n",
        "        ]\n",
        "\n",
        "classifiers = [\n",
        "    MultinomialNB(),\n",
        "    LinearSVC(),\n",
        "    LogisticRegression(),\n",
        "    RandomForestClassifier(),\n",
        "    MLPClassifier()\n",
        "]\n",
        "\n",
        "parameters = [\n",
        "              {'alpha': (1e-2, 1e-3, 1e-4)},\n",
        "              {'C': (np.logspace(-5, 1, 5))},\n",
        "              {'C': (np.logspace(-5, 1, 5))},\n",
        "              {'max_depth': (1, 2, 3, 4, 5)},\n",
        "              {'alpha': (1e-2, 1e-3), \"max_iter\": [1000, 2000, 3000]}\n",
        "             ]\n",
        "\n",
        "\n",
        "rows = []\n",
        "cms = []\n",
        "for name, classifier, params in zip(names, classifiers, parameters):\n",
        "    gs_clf = GridSearchCV(classifier, param_grid=params, n_jobs=-1)\n",
        "    clf = gs_clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    \n",
        "    predictions = clf.predict(X_test)\n",
        "    \n",
        "    cm = confusion_matrix(predictions, y_test.tolist())\n",
        "    cms.append((name, cm, clf.classes_))\n",
        "   \n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\"{} score: {}\".format(name, clf.best_score_))\n",
        "    print(\"Grid scores on test set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "        \n",
        "        row = (name, mean, str(params))\n",
        "        rows.append(row)\n",
        "        \n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QI6SFV0JI0Pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for name, conf_matrix, classes in cms:\n",
        "  plot_confusion_matrix(conf_matrix, classes, title=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F4iXete0k-Fk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(rows, columns=[\"algorithm\", \"score\", \"params\"])\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWdRa02pdXXC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plot_data = []\n",
        "\n",
        "for name in names:\n",
        "  bar = go.Bar(\n",
        "      x = results[results['algorithm'] == name]['params'].tolist(),\n",
        "      y = results[results['algorithm'] == name]['score'].tolist(),\n",
        "      name = name\n",
        "  )\n",
        "  plot_data.append(bar)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "fig = go.Figure(data=plot_data)\n",
        "py.plot(fig, filename=\"base-bar-results.html\")\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(filename=\"./base-bar-results.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v6x4Pd3JXwgb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Upx0MWduqtdO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_RfM5yeX7uA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
